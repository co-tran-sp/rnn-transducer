{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "commercial-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "latin-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_inputs_to_attention_format_fixed(target_text,params):\n",
    "    #train_inputs = np.asarray(inputs)\n",
    "    #train_inputs = process(train_inputs)\n",
    "    #train_seq_len = train_inputs.shape[0]\n",
    "    original = target_text.lower()\n",
    "    target_text = '<start>'+ target_text.lower() + '<end>'\n",
    "    target_text = target_text.lower().split(' ')\n",
    "    # Transform char into index\n",
    "    targets = np.asarray([params.dictionary[x] for x in target_text])\n",
    "    return  targets, original\n",
    "\n",
    "def get_dictionary_fixed(label_dir, params):\n",
    "    with open(label_dir, 'r') as txt:\n",
    "        data = txt.readlines()\n",
    "    data = data[:params.number_sentence]\n",
    "    word_list = []\n",
    "    raw_text = []\n",
    "    for line in data:\n",
    "        sentence = re.sub(r\"[^a-zA-Z]+\", ' ', line)\n",
    "        words_vector = []\n",
    "        sentence = sentence.split(' ')\n",
    "        for word in sentence:\n",
    "            if word != '':\n",
    "                words_vector.append(word.lower())\n",
    "                word_list.append(word.lower())\n",
    "        raw_text.append(words_vector)\n",
    "    word_list = pd.Series(word_list).unique()\n",
    "    dict = {'<start>': 1, '<end>': 2}\n",
    "    for word in word_list:\n",
    "        if word not in dict:\n",
    "            dict[word] = len(dict) + 1\n",
    "    return dict\n",
    "\n",
    "def pad_sequences(sequences, maxlen=None, dtype=np.float32,\n",
    "                  padding='post', truncating='post', value=0.):\n",
    "    '''Pads each sequence to the same length: the length of the longest\n",
    "    sequence.\n",
    "        If maxlen is provided, any sequence longer than maxlen is truncated to\n",
    "        maxlen. Truncation happens off either the beginning or the end\n",
    "        (default) of the sequence. Supports post-padding (default) and\n",
    "        pre-padding.\n",
    "        Args:\n",
    "            sequences: list of lists where each element is a sequence\n",
    "            maxlen: int, maximum length\n",
    "            dtype: type to cast the resulting sequence.\n",
    "            padding: 'pre' or 'post', pad either before or after each sequence.\n",
    "            truncating: 'pre' or 'post', remove values from sequences larger\n",
    "            than maxlen either in the beginning or in the end of the sequence\n",
    "            value: float, value to pad the sequences to the desired value.\n",
    "        Returns\n",
    "            x: numpy array with dimensions (number_of_sequences, maxlen)\n",
    "            lengths: numpy array with the original sequence lengths\n",
    "    '''\n",
    "    lengths = np.asarray([len(s) for s in sequences], dtype=np.int64)\n",
    "\n",
    "    nb_samples = len(sequences)\n",
    "    if maxlen is None:\n",
    "        maxlen = np.max(lengths)\n",
    "\n",
    "    # take the sample shape from the first non empty sequence\n",
    "    # checking for consistency in the main loop below.\n",
    "    sample_shape = tuple()\n",
    "    for s in sequences:\n",
    "        if len(s) > 0:\n",
    "            sample_shape = np.asarray(s).shape[1:]\n",
    "            break\n",
    "\n",
    "    x = (np.ones((nb_samples, maxlen) + sample_shape) * value).astype(dtype)\n",
    "    for idx, s in enumerate(sequences):\n",
    "        if len(s) == 0:\n",
    "            continue  # empty list was found\n",
    "        if truncating == 'pre':\n",
    "            trunc = s[-maxlen:]\n",
    "        elif truncating == 'post':\n",
    "            trunc = s[:maxlen]\n",
    "        else:\n",
    "            raise ValueError('Truncating type \"%s\" not understood' % truncating)\n",
    "\n",
    "        # check `trunc` has expected shape\n",
    "        trunc = np.asarray(trunc, dtype=dtype)\n",
    "        if trunc.shape[1:] != sample_shape:\n",
    "            raise ValueError('Shape of sample %s of sequence at position %s is different from expected shape %s' %\n",
    "                             (trunc.shape[1:], idx, sample_shape))\n",
    "\n",
    "        if padding == 'post':\n",
    "            x[idx, :len(trunc)] = trunc\n",
    "        elif padding == 'pre':\n",
    "            x[idx, -len(trunc):] = trunc\n",
    "        else:\n",
    "            raise ValueError('Padding type \"%s\" not understood' % padding)\n",
    "    return x, lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cross-toilet",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'unidecode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-cd6905db844e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/rnn-transducer/model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0munidecode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspeechbrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransducer_loss\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTransducerLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'unidecode'"
     ]
    }
   ],
   "source": [
    "from model import *\n",
    "from utils import *\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from jiwer import wer as jwer\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "class parameters():\n",
    "\n",
    "    number_sentence = 8854\n",
    "    num_layers = 4\n",
    "    d_model = 256\n",
    "    dff = 1024\n",
    "    num_heads = 16\n",
    "    input_vocab_size = 0\n",
    "    target_vocab_size = 0\n",
    "    dropout_rate = 0.1\n",
    "    freq = 100\n",
    "    feature = 1\n",
    "    data = 'eeg'\n",
    "    seed = 1234\n",
    "    n_batches = 10\n",
    "    epochs = 50\n",
    "    prenet = True\n",
    "\n",
    "\"\"\"\n",
    "Get dataset and parameters\n",
    "\"\"\"\n",
    "tf.random.set_seed(params.seed)\n",
    "params.dictionary = get_dictionary_fixed('sentences.txt',params)\n",
    "label_dir = 'sentences.txt'\n",
    "target_set = []\n",
    "with open(label_dir, 'r') as txt:\n",
    "    data = txt.readlines()\n",
    "    for line in data:\n",
    "        sentence = re.sub(r\"[^a-zA-Z]+\", ' ', line)\n",
    "        target_set.append(convert_inputs_to_attention_format_fixed(sentence,params)[0])\n",
    "input_set = np.random.rand(8854, 700, 13)\n",
    "input_set, inp_seq_len = pad_sequences(input_set, dtype=np.float32)\n",
    "target_set, target_seq_len = pad_sequences(target_set, dtype=np.int64)\n",
    "params.target_vocab_size = len(params.dictionary)+1\n",
    "params.max_length = len(target_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-processor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
